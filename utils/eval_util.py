import pytrec_eval

def calculate_retrieval_metrics(results, qrels, k_values=[1, 5, 10, 25, 50, 100]):
    # https://github.com/beir-cellar/beir/blob/f062f038c4bfd19a8ca942a9910b1e0d218759d4/beir/retrieval/evaluation.py#L66
    # follow evaluation from BEIR, which is just using the trec eval
    ndcg = {}
    _map = {}
    recall = {}
    precision = {}
    mrr = {"MRR": 0}

    for k in k_values:
        ndcg[f"NDCG@{k}"] = 0.0
        _map[f"MAP@{k}"] = 0.0
        recall[f"Recall@{k}"] = 0.0
        precision[f"P@{k}"] = 0.0

    map_string = "map_cut." + ",".join([str(k) for k in k_values])
    ndcg_string = "ndcg_cut." + ",".join([str(k) for k in k_values])
    recall_string = "recall." + ",".join([str(k) for k in k_values])
    precision_string = "P." + ",".join([str(k) for k in k_values])

    # https://github.com/cvangysel/pytrec_eval/blob/master/examples/simple_cut.py
    # qrels = {qid: {'pid': [0/1] (relevance label)}}
    # results = {qid: {'pid': float (retriever score)}}
    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {map_string, ndcg_string, recall_string, precision_string, "recip_rank"})
    scores = evaluator.evaluate(results)

    for query_id in scores.keys():
        for k in k_values:
            ndcg[f"NDCG@{k}"] += scores[query_id]["ndcg_cut_" + str(k)]
            _map[f"MAP@{k}"] += scores[query_id]["map_cut_" + str(k)]
            recall[f"Recall@{k}"] += scores[query_id]["recall_" + str(k)]
            precision[f"P@{k}"] += scores[query_id]["P_" + str(k)]
        mrr["MRR"] += scores[query_id]["recip_rank"]

    for k in k_values:
        ndcg[f"NDCG@{k}"] = round(ndcg[f"NDCG@{k}"] / len(scores), 5)
        _map[f"MAP@{k}"] = round(_map[f"MAP@{k}"] / len(scores), 5)
        recall[f"Recall@{k}"] = round(recall[f"Recall@{k}"] / len(scores), 5)
        precision[f"P@{k}"] = round(precision[f"P@{k}"] / len(scores), 5)
    mrr["MRR"] = round(mrr["MRR"] / len(scores), 5)

    # oracle reranker evaluation
    sorted_ids = {}
    top_100_ids = {}
    for query_id in results.keys():
        sorted_ids[query_id] = sorted(results[query_id].keys(), key=lambda x: results[query_id][x], reverse=True)
        top_100_ids[query_id] = set(sorted_ids[query_id][:100])
    oracle_results = {}
    for query_id in results.keys():
        oracle_results[query_id] = {}
        for doc_id in results[query_id].keys():
            if doc_id in top_100_ids[query_id] and query_id in qrels and doc_id in qrels[query_id]: # a doc is both top 100 and also in ground truth
                oracle_results[query_id][doc_id] = qrels[query_id][doc_id] # extract the score from ground truth
            else:
                oracle_results[query_id][doc_id] = 0
    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {map_string, ndcg_string, recall_string, precision_string, "recip_rank"})
    oracle_scores = evaluator.evaluate(oracle_results)
    oracle_ndcg = {}
    for k in k_values:
        oracle_ndcg[f"Oracle NDCG@{k}"] = 0.0
    for query_id in oracle_scores.keys():
        for k in k_values:
            oracle_ndcg[f"Oracle NDCG@{k}"] += oracle_scores[query_id]["ndcg_cut_" + str(k)]
    for k in k_values:
        oracle_ndcg[f"Oracle NDCG@{k}"] = round(oracle_ndcg[f"Oracle NDCG@{k}"] / len(oracle_scores), 5)
    

    output = {**ndcg, **_map, **recall, **precision, **mrr, **oracle_ndcg}
    print(output)

    return output

def calculate_retrieval_metrics_per_query(results, qrels, k_values=[1, 5, 10, 25, 50, 100]):
    """
    Calculate per-query retrieval metrics.
    
    Args:
        results: {qid: {'pid': float (retriever score)}}
        qrels: {qid: {'pid': [0/1] (relevance label)}}
        k_values: List of k values for metrics
    
    Returns:
        per_query_metrics: {qid: {metric_name: score}}
    """
    map_string = "map_cut." + ",".join([str(k) for k in k_values])
    ndcg_string = "ndcg_cut." + ",".join([str(k) for k in k_values])
    recall_string = "recall." + ",".join([str(k) for k in k_values])
    precision_string = "P." + ",".join([str(k) for k in k_values])

    # Evaluate with pytrec_eval
    evaluator = pytrec_eval.RelevanceEvaluator(
        qrels, 
        {map_string, ndcg_string, recall_string, precision_string, "recip_rank"}
    )
    scores = evaluator.evaluate(results)

    # Store per-query metrics
    per_query_metrics = {}
    for query_id in scores.keys():
        per_query_metrics[query_id] = {}
        for k in k_values:
            per_query_metrics[query_id][f"NDCG@{k}"] = scores[query_id]["ndcg_cut_" + str(k)]
            per_query_metrics[query_id][f"MAP@{k}"] = scores[query_id]["map_cut_" + str(k)]
            per_query_metrics[query_id][f"Recall@{k}"] = scores[query_id]["recall_" + str(k)]
            per_query_metrics[query_id][f"P@{k}"] = scores[query_id]["P_" + str(k)]
        per_query_metrics[query_id]["MRR"] = scores[query_id]["recip_rank"]

    # Oracle reranker evaluation
    top_100_ids = {}
    for query_id in results.keys():
        sorted_ids = sorted(results[query_id].keys(), key=lambda x: results[query_id][x], reverse=True)
        top_100_ids[query_id] = set(sorted_ids[:100])
    
    oracle_results = {}
    for query_id in results.keys():
        oracle_results[query_id] = {}
        for doc_id in results[query_id].keys():
            if doc_id in top_100_ids[query_id] and query_id in qrels and doc_id in qrels[query_id]:
                oracle_results[query_id][doc_id] = qrels[query_id][doc_id]
            else:
                oracle_results[query_id][doc_id] = 0
    
    oracle_evaluator = pytrec_eval.RelevanceEvaluator(qrels, {ndcg_string})
    oracle_scores = oracle_evaluator.evaluate(oracle_results)
    
    # Add oracle scores to per-query metrics
    for query_id in oracle_scores.keys():
        if query_id not in per_query_metrics:
            per_query_metrics[query_id] = {}
        for k in k_values:
            per_query_metrics[query_id][f"Oracle NDCG@{k}"] = oracle_scores[query_id]["ndcg_cut_" + str(k)]

    return per_query_metrics